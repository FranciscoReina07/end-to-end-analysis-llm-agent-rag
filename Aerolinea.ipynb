{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "00b53022",
      "metadata": {},
      "source": [
        "# Proyecto Aerya: EDA, metricas, embeddings, agente y FastApi\n",
        "\n",
        "Objetivo:\n",
        "- EDA \n",
        "- Metricas de impacto para la aerolinea con scikit-learn.\n",
        "- Embeddings locales con Ollama.\n",
        "- Agente con LangGraph usando OpenAI (API).\n",
        "- End point con FastApi\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df0bc8f3",
      "metadata": {},
      "source": [
        "## 1. Configuracion y rutas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bc221ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ENV CONFIG (primera celda)\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\"OPENAI_API_KEY no esta definida en .env\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79e26884",
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import json\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_colwidth\", 120)\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        "    force=True,\n",
        ")\n",
        "logger = logging.getLogger(\"aerya\")\n",
        "\n",
        "BASE_DIR = Path(\"data\")\n",
        "THREADS_PATH = BASE_DIR / \"Threads.json\"\n",
        "MESSAGES_PATH = BASE_DIR / \"Messages.json\"\n",
        "\n",
        "if not BASE_DIR.exists():\n",
        "    raise FileNotFoundError(f\"La carpeta de datos no existe: {BASE_DIR.resolve()}\")\n",
        "if not THREADS_PATH.exists() or not MESSAGES_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Faltan archivos JSON en {BASE_DIR.resolve()}\")\n",
        "\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "class EmbeddingError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "class ModelTrainingError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "class AgentError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "def validar_columnas(df: pd.DataFrame, required: set[str], contexto: str) -> None:\n",
        "    missing = required - set(df.columns)\n",
        "    if missing:\n",
        "        raise DataValidationError(f\"Faltan columnas en {contexto}: {missing}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "469f2ed3",
      "metadata": {},
      "source": [
        "## 2. Carga y estructura"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08d7b41e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cargar_datos(ruta_threads: Path, ruta_messages: Path) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    if not ruta_threads.exists() or not ruta_messages.exists():\n",
        "        raise FileNotFoundError(\"No se encuentran los archivos JSON.\")\n",
        "\n",
        "    logger.info(\"Cargando datos desde %s y %s ...\", ruta_threads, ruta_messages)\n",
        "\n",
        "    try:\n",
        "        df_threads = pd.read_json(ruta_threads)\n",
        "        df_messages = pd.read_json(ruta_messages)\n",
        "    except ValueError as exc:\n",
        "        raise DataValidationError(f\"JSON malformado o estructura incompatible: {exc}\") from exc\n",
        "\n",
        "    if df_threads.empty or df_messages.empty:\n",
        "        raise DataValidationError(\"Alguno de los JSON esta vacio.\")\n",
        "\n",
        "    logger.info(\n",
        "        \"Datos cargados: threads=%d filas, messages=%d filas\",\n",
        "        len(df_threads),\n",
        "        len(df_messages),\n",
        "    )\n",
        "    return df_threads, df_messages\n",
        "\n",
        "\n",
        "def mostrar_estructura(df: pd.DataFrame, nombre: str, head_rows: int = 3) -> None:\n",
        "    print(f\"DATAFRAME: {nombre}\")\n",
        "    print(f\"Dimensiones: {df.shape}\")\n",
        "    print(\"\\nColumnas\")\n",
        "    print(df.columns.tolist())\n",
        "    print(\"\\nResumen de tipos y nulos\")\n",
        "    df.info()\n",
        "    print(f\"\\nHead ({head_rows})\")\n",
        "    print(df.head(head_rows))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "\n",
        "try:\n",
        "    df_threads, df_messages = cargar_datos(THREADS_PATH, MESSAGES_PATH)\n",
        "    mostrar_estructura(df_threads, \"df_threads\")\n",
        "    mostrar_estructura(df_messages, \"df_messages\")\n",
        "except (FileNotFoundError, DataValidationError) as exc:\n",
        "    logger.error(\"Error durante la carga: %s\", exc)\n",
        "except Exception as exc:\n",
        "    logger.error(\"Error inesperado en carga: %s\", exc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea8b51d1",
      "metadata": {},
      "source": [
        "## 3. EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fdd76ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _get_hashable_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    temp_df = df.copy()\n",
        "    object_cols = temp_df.select_dtypes(include=[\"object\"]).columns\n",
        "    temp_df[object_cols] = temp_df[object_cols].astype(str).replace(\"nan\", pd.NA)\n",
        "    return temp_df\n",
        "\n",
        "\n",
        "def _profile_missing(df: pd.DataFrame, top_n: int = 15) -> pd.DataFrame:\n",
        "    missing = df.isna().sum().sort_values(ascending=False)\n",
        "    return missing[missing > 0].head(top_n).to_frame(name=\"missing_count\")\n",
        "\n",
        "\n",
        "def _profile_uniques(df: pd.DataFrame, top_n: int = 15) -> pd.DataFrame:\n",
        "    safe_df = _get_hashable_df(df)\n",
        "    uniques = safe_df.nunique(dropna=True).sort_values(ascending=False).head(top_n)\n",
        "    return uniques.to_frame(name=\"unique_values\")\n",
        "\n",
        "\n",
        "def _safe_duplicated_count(df: pd.DataFrame) -> int:\n",
        "    safe_df = _get_hashable_df(df)\n",
        "    return safe_df.duplicated().sum()\n",
        "\n",
        "\n",
        "def analizar_dataset(df: pd.DataFrame, nombre: str) -> None:\n",
        "    print(f\"ANALISIS DEL DATASET: {nombre}\")\n",
        "    print(f\"Dimensiones: {df.shape[0]} filas x {df.shape[1]} columnas\")\n",
        "\n",
        "    n_duplicados = _safe_duplicated_count(df)\n",
        "    perc_duplicados = (n_duplicados / len(df)) * 100 if len(df) else 0\n",
        "    print(f\"Duplicados: {n_duplicados} ({perc_duplicados:.2f}%)\")\n",
        "\n",
        "    print(\"\\nTop 15 columnas con faltantes\")\n",
        "    missing_df = _profile_missing(df, top_n=15)\n",
        "    if missing_df.empty:\n",
        "        print(\"No hay valores faltantes.\")\n",
        "    else:\n",
        "        print(missing_df)\n",
        "\n",
        "    print(\"\\nTop 15 columnas con valores unicos\")\n",
        "    print(_profile_uniques(df, top_n=15))\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "\n",
        "analizar_dataset(df_threads, \"df_threads\")\n",
        "analizar_dataset(df_messages, \"df_messages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1a8b7e5",
      "metadata": {},
      "source": [
        "## 4. Tabla maestra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff2853d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _to_snake_case(name: str) -> str:\n",
        "    name = name.strip()\n",
        "    name = re.sub(r\"[\\s\\-]+\", \"_\", name)\n",
        "    name = re.sub(r\"[^0-9a-zA-Z_]+\", \"\", name)\n",
        "    name = re.sub(r\"_+\", \"_\", name)\n",
        "    return name.lower()\n",
        "\n",
        "\n",
        "def _normalize_text_content(value: Any) -> str:\n",
        "    if value is None:\n",
        "        return \"\"\n",
        "    if isinstance(value, float) and pd.isna(value):\n",
        "        return \"\"\n",
        "    if isinstance(value, (list, tuple, set)):\n",
        "        return \"; \".join(str(item) for item in value)\n",
        "    if isinstance(value, dict):\n",
        "        return json.dumps(value, ensure_ascii=False)\n",
        "    return str(value).strip()\n",
        "\n",
        "\n",
        "def normalizar_columnas(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df.columns = [_to_snake_case(col) for col in df.columns]\n",
        "    return df\n",
        "\n",
        "\n",
        "def _extract_date_value(value: Any) -> Optional[str]:\n",
        "    if isinstance(value, dict) and \"$date\" in value:\n",
        "        return value[\"$date\"]\n",
        "    if value is None or (isinstance(value, float) and pd.isna(value)):\n",
        "        return None\n",
        "    return str(value)\n",
        "\n",
        "\n",
        "def crear_tabla_maestra(df_t: pd.DataFrame, df_m: pd.DataFrame) -> pd.DataFrame:\n",
        "    df_t = normalizar_columnas(df_t)\n",
        "    df_m = normalizar_columnas(df_m)\n",
        "\n",
        "    validar_columnas(df_t, {\"thread_id\"}, \"threads\")\n",
        "    validar_columnas(df_m, {\"thread_id\", \"content\"}, \"messages\")\n",
        "\n",
        "    if \"created_at\" in df_t.columns:\n",
        "        df_t[\"created_at\"] = pd.to_datetime(\n",
        "            df_t[\"created_at\"].map(_extract_date_value), errors=\"coerce\", utc=True\n",
        "        )\n",
        "\n",
        "    col_fecha_msg = \"created_at\" if \"created_at\" in df_m.columns else \"createdat\"\n",
        "    if col_fecha_msg in df_m.columns:\n",
        "        df_m[\"msg_timestamp\"] = pd.to_datetime(\n",
        "            df_m[col_fecha_msg].map(_extract_date_value), errors=\"coerce\", utc=True\n",
        "        )\n",
        "    else:\n",
        "        df_m[\"msg_timestamp\"] = pd.NaT\n",
        "\n",
        "    df_m = df_m.sort_values(by=[\"thread_id\", \"msg_timestamp\"])\n",
        "    df_m[\"clean_content\"] = df_m[\"content\"].apply(_normalize_text_content)\n",
        "\n",
        "    df_grouped = df_m.groupby(\"thread_id\").agg(\n",
        "        full_conversation=(\"clean_content\", lambda x: \" | \".join(x[x != \"\"])),\n",
        "        msg_count=(\"clean_content\", \"count\"),\n",
        "        first_msg=(\"msg_timestamp\", \"min\"),\n",
        "        last_msg=(\"msg_timestamp\", \"max\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    if df_grouped.empty:\n",
        "        raise DataValidationError(\"No se pudieron agrupar mensajes por thread_id.\")\n",
        "\n",
        "    threads_antes = len(df_t)\n",
        "    df_master = pd.merge(df_t, df_grouped, on=\"thread_id\", how=\"inner\")\n",
        "    threads_perdidos = threads_antes - len(df_master)\n",
        "\n",
        "    if df_master.empty:\n",
        "        raise DataValidationError(\"La tabla maestra quedo vacia tras el merge.\")\n",
        "\n",
        "    if threads_perdidos > 0:\n",
        "        logger.warning(\n",
        "            \"Merge: %d threads sin mensajes fueron descartados (%d -> %d)\",\n",
        "            threads_perdidos, threads_antes, len(df_master),\n",
        "        )\n",
        "\n",
        "    df_master[\"duration_minutes\"] = (\n",
        "        (df_master[\"last_msg\"] - df_master[\"first_msg\"]).dt.total_seconds() / 60\n",
        "    ).fillna(0)\n",
        "\n",
        "    return df_master\n",
        "\n",
        "\n",
        "try:\n",
        "    df_master = crear_tabla_maestra(df_threads, df_messages)\n",
        "    logger.info(\"Tabla maestra creada correctamente - Dimensiones: %s\", df_master.shape)\n",
        "    print(df_master[[\"thread_id\", \"msg_count\", \"duration_minutes\"]].head(3))\n",
        "except DataValidationError as exc:\n",
        "    logger.error(\"Error de validacion en tabla maestra: %s\", exc)\n",
        "except Exception as exc:\n",
        "    logger.error(\"Error en la tabla maestra: %s\", exc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b736ae38",
      "metadata": {},
      "source": [
        "## 5. Graficas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89254fbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def _plot_escalamiento_por_categoria(df: pd.DataFrame, col: str, top_n: int = 10) -> None:\n",
        "    if col not in df.columns or \"escalate_conversation\" not in df.columns:\n",
        "        print(f\"No se puede graficar por {col}.\")\n",
        "        return\n",
        "\n",
        "    tmp = df[[col, \"escalate_conversation\"]].copy()\n",
        "    tmp[\"escalate_conversation\"] = tmp[\"escalate_conversation\"].fillna(0).astype(int)\n",
        "\n",
        "    rate = (\n",
        "        tmp.groupby(col)[\"escalate_conversation\"]\n",
        "        .mean()\n",
        "        .sort_values(ascending=False)\n",
        "        .head(top_n)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.barplot(data=rate, x=\"escalate_conversation\", y=col, color=\"#1f77b4\")\n",
        "    plt.title(f\"Tasa de escalamiento por {col} (top {top_n})\")\n",
        "    plt.xlabel(\"Tasa de escalamiento\")\n",
        "    plt.ylabel(col)\n",
        "    plt.xlim(0, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def _plot_mensajes_vs_escalamiento(df: pd.DataFrame) -> None:\n",
        "    if \"msg_count\" not in df.columns or \"escalate_conversation\" not in df.columns:\n",
        "        print(\"No se puede graficar mensajes vs escalamiento.\")\n",
        "        return\n",
        "\n",
        "    tmp = df[[\"msg_count\", \"escalate_conversation\"]].copy()\n",
        "    tmp[\"escalate_conversation\"] = tmp[\"escalate_conversation\"].fillna(0).astype(int)\n",
        "\n",
        "    bins = [0, 2, 5, 10, 20, 50, 100, np.inf]\n",
        "    labels = [\"0-2\", \"3-5\", \"6-10\", \"11-20\", \"21-50\", \"51-100\", \"100+\"]\n",
        "    tmp[\"msg_bucket\"] = pd.cut(tmp[\"msg_count\"], bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "    rate = tmp.groupby(\"msg_bucket\")[\"escalate_conversation\"].mean().reset_index()\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.barplot(data=rate, x=\"msg_bucket\", y=\"escalate_conversation\", color=\"#ff7f0e\")\n",
        "    plt.title(\"Tasa de escalamiento por volumen de mensajes\")\n",
        "    plt.xlabel(\"Mensajes por hilo\")\n",
        "    plt.ylabel(\"Tasa de escalamiento\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "_plot_escalamiento_por_categoria(df_master, \"status\", top_n=8)\n",
        "_plot_mensajes_vs_escalamiento(df_master)\n",
        "\n",
        "\n",
        "def metricas_impacto_aerolinea(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df.empty:\n",
        "        raise DataValidationError(\"No hay datos para calcular metricas.\")\n",
        "\n",
        "    required = {\"escalate_conversation\", \"duration_minutes\", \"msg_count\", \"status\"}\n",
        "    available = required.intersection(df.columns)\n",
        "\n",
        "    metrics: Dict[str, Any] = {}\n",
        "    if \"escalate_conversation\" in available:\n",
        "        target = df[\"escalate_conversation\"].fillna(0).astype(int)\n",
        "        metrics[\"tasa_escalamiento\"] = target.mean()\n",
        "\n",
        "    if \"duration_minutes\" in available:\n",
        "        metrics[\"duracion_promedio_min\"] = df[\"duration_minutes\"].mean()\n",
        "        metrics[\"duracion_p90_min\"] = df[\"duration_minutes\"].quantile(0.90)\n",
        "\n",
        "    if \"msg_count\" in available:\n",
        "        metrics[\"mensajes_promedio\"] = df[\"msg_count\"].mean()\n",
        "        metrics[\"mensajes_p90\"] = df[\"msg_count\"].quantile(0.90)\n",
        "\n",
        "    if \"status\" in available:\n",
        "        metrics[\"estado_top\"] = json.dumps(df[\"status\"].value_counts().head(5).to_dict())\n",
        "\n",
        "    metrics_df = pd.DataFrame([metrics])\n",
        "    print(\"Metricas de impacto (resumen)\")\n",
        "    print(metrics_df)\n",
        "    return metrics_df\n",
        "\n",
        "\n",
        "_ = metricas_impacto_aerolinea(df_master)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "648701ac",
      "metadata": {},
      "source": [
        "## 6. Embeddings locales con Ollama + modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "198588de",
      "metadata": {},
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import joblib\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        ")\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "import warnings\n",
        "\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# CONFIG\n",
        "\n",
        "EMBEDDINGS_CACHE_PATH = Path(\"outputs\") / \"embeddings_cache.parquet\"\n",
        "MODEL_DIR = Path(\"models\")\n",
        "MODEL_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "MAX_CHARS = 1000\n",
        "CHUNK_SIZE = 500\n",
        "\n",
        "\n",
        "# DATA PREP\n",
        "\n",
        "def _preparar_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    required = {\"full_conversation\", \"escalate_conversation\"}\n",
        "    missing = required - set(df.columns)\n",
        "    if missing:\n",
        "        raise DataValidationError(f\"Faltan columnas: {missing}\")\n",
        "\n",
        "    df_model = df.copy()\n",
        "\n",
        "    df_model[\"full_conversation\"] = (\n",
        "        df_model[\"full_conversation\"]\n",
        "        .fillna(\"\")\n",
        "        .str.slice(0, MAX_CHARS)\n",
        "    )\n",
        "\n",
        "    df_model = df_model[df_model[\"full_conversation\"].str.len() > 0]\n",
        "    df_model[\"target\"] = df_model[\"escalate_conversation\"].fillna(0).astype(int)\n",
        "\n",
        "    return df_model\n",
        "\n",
        "\n",
        "# TEXT UTILS\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE) -> List[str]:\n",
        "    words = text.split()\n",
        "    return [\n",
        "        \" \".join(words[i:i + chunk_size])\n",
        "        for i in range(0, len(words), chunk_size)\n",
        "    ]\n",
        "\n",
        "\n",
        "def _hash_text(text: str) -> str:\n",
        "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "\n",
        "def _build_chunks_for_texts(texts: List[str]) -> Tuple[List[str], List[List[str]]]:\n",
        "    all_chunks: List[str] = []\n",
        "    text_to_hashes: List[List[str]] = []\n",
        "\n",
        "    for text in texts:\n",
        "        chunks = chunk_text(text)\n",
        "        hashes = [_hash_text(chunk) for chunk in chunks]\n",
        "        all_chunks.extend(chunks)\n",
        "        text_to_hashes.append(hashes)\n",
        "\n",
        "    return all_chunks, text_to_hashes\n",
        "\n",
        "\n",
        "# CACHE\n",
        "\n",
        "def _load_embeddings_cache(\n",
        "    path: Path = EMBEDDINGS_CACHE_PATH\n",
        ") -> Dict[str, List[float]]:\n",
        "\n",
        "    if not path.exists():\n",
        "        return {}\n",
        "\n",
        "    cache_df = pd.read_parquet(path)\n",
        "    return dict(zip(cache_df[\"text_hash\"], cache_df[\"embedding\"]))\n",
        "\n",
        "\n",
        "def _save_embeddings_cache(\n",
        "    cache: Dict[str, List[float]],\n",
        "    path: Path = EMBEDDINGS_CACHE_PATH\n",
        ") -> None:\n",
        "\n",
        "    cache_df = pd.DataFrame({\n",
        "        \"text_hash\": list(cache.keys()),\n",
        "        \"embedding\": list(cache.values())\n",
        "    })\n",
        "\n",
        "    path.parent.mkdir(exist_ok=True)\n",
        "    cache_df.to_parquet(path, index=False)\n",
        "    logger.info(\"Cache actualizado: %s\", path)\n",
        "\n",
        "\n",
        "# EMBEDDINGS CORE\n",
        "\n",
        "def _batch_iter(items: List[str], batch_size: int) -> Iterable[List[str]]:\n",
        "    for i in range(0, len(items), batch_size):\n",
        "        yield items[i:i + batch_size]\n",
        "\n",
        "\n",
        "def generate_embeddings_with_cache(\n",
        "    texts: List[str],\n",
        "    model_name: str = \"nomic-embed-text\",\n",
        "    batch_size: int = 64,\n",
        ") -> List[List[float]]:\n",
        "\n",
        "    if not texts:\n",
        "        raise EmbeddingError(\"No hay textos para vectorizar.\")\n",
        "\n",
        "    embedder = OllamaEmbeddings(model=model_name)\n",
        "    cache = _load_embeddings_cache()\n",
        "\n",
        "    all_chunks, text_to_hashes = _build_chunks_for_texts(texts)\n",
        "    unique_chunks = {}\n",
        "    for chunk in all_chunks:\n",
        "        chunk_hash = _hash_text(chunk)\n",
        "        if chunk_hash not in unique_chunks and chunk_hash not in cache:\n",
        "            unique_chunks[chunk_hash] = chunk\n",
        "\n",
        "    missing_hashes = list(unique_chunks.keys())\n",
        "    missing_chunks = [unique_chunks[h] for h in missing_hashes]\n",
        "\n",
        "    if missing_chunks:\n",
        "        for batch in tqdm(\n",
        "            _batch_iter(missing_chunks, batch_size),\n",
        "            desc=\"Vectorizando chunks\",\n",
        "        ):\n",
        "            batch_embeddings = embedder.embed_documents(batch)\n",
        "            for chunk_text, emb in zip(batch, batch_embeddings):\n",
        "                cache[_hash_text(chunk_text)] = emb\n",
        "\n",
        "        _save_embeddings_cache(cache)\n",
        "    else:\n",
        "        logger.info(\"Todos los chunks estaban en cache.\")\n",
        "\n",
        "    embeddings: List[List[float]] = []\n",
        "    for hashes in text_to_hashes:\n",
        "        chunk_embs = [cache[h] for h in hashes]\n",
        "        embeddings.append(np.mean(chunk_embs, axis=0).tolist())\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "# ML\n",
        "\n",
        "def train_model(\n",
        "    X_train: List[List[float]],\n",
        "    y_train: pd.Series\n",
        ") -> LogisticRegression:\n",
        "\n",
        "    if len(np.unique(y_train)) < 2:\n",
        "        raise ModelTrainingError(\"Target con una sola clase.\")\n",
        "\n",
        "    clf = LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def save_model(model: LogisticRegression, version: str = \"v1\") -> Path:\n",
        "    model_path = MODEL_DIR / f\"model_{version}.joblib\"\n",
        "    joblib.dump(model, model_path)\n",
        "    logger.info(\"Modelo guardado: %s\", model_path)\n",
        "    return model_path\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test) -> None:\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print(\"\\nREPORTE\")\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "        print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"\\nMATRIZ CONFUSION\")\n",
        "    print(cm)\n",
        "    print(f\"Falsos Negativos: {cm[1][0]}\")\n",
        "\n",
        "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "    print(f\"PR-AUC: {average_precision_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "    fn_cost = 5\n",
        "    fp_cost = 1\n",
        "    total_cost = cm[1][0]*fn_cost + cm[0][1]*fp_cost\n",
        "    print(f\"Costo Esperado: {total_cost}\")\n",
        "\n",
        "\n",
        "# PIPELINE\n",
        "\n",
        "def entrenar_clasificador_embeddings(\n",
        "    df: pd.DataFrame,\n",
        "    model_name: str = \"nomic-embed-text\"\n",
        "):\n",
        "\n",
        "    df_model = _preparar_dataset(df)\n",
        "\n",
        "    texts = df_model[\"full_conversation\"].tolist()\n",
        "\n",
        "    embeddings = generate_embeddings_with_cache(\n",
        "        texts,\n",
        "        model_name=model_name,\n",
        "        batch_size=64,\n",
        "    )\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        embeddings,\n",
        "        df_model[\"target\"],\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df_model[\"target\"]\n",
        "    )\n",
        "\n",
        "    clf = train_model(X_train, y_train)\n",
        "    save_model(clf)\n",
        "\n",
        "    evaluate_model(clf, X_test, y_test)\n",
        "\n",
        "    return clf, X_test, y_test\n",
        "\n",
        "# RUN\n",
        "\n",
        "try:\n",
        "    clf_model, X_test, y_test = entrenar_clasificador_embeddings(df_master)\n",
        "except (EmbeddingError, ModelTrainingError, DataValidationError) as exc:\n",
        "    logger.error(\"Error controlado: %s\", exc)\n",
        "except Exception as exc:\n",
        "    logger.error(\"Error inesperado: %s\", exc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee8c3aaa",
      "metadata": {},
      "source": [
        "## 7. Agente LangGraph con OpenAI \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6596b6e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# \n",
        "# --- LLM Singleton ---\n",
        "LLM = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def construir_resumen_contexto(df: pd.DataFrame) -> str:\n",
        "    resumen = []\n",
        "    if \"escalate_conversation\" in df.columns:\n",
        "        tasa = df[\"escalate_conversation\"].fillna(0).astype(int).mean()\n",
        "        resumen.append(f\"Tasa de escalamiento: {tasa:.3f}\")\n",
        "    if \"duration_minutes\" in df.columns:\n",
        "        resumen.append(f\"Duracion promedio (min): {df['duration_minutes'].mean():.2f}\")\n",
        "    if \"msg_count\" in df.columns:\n",
        "        resumen.append(f\"Mensajes promedio: {df['msg_count'].mean():.2f}\")\n",
        "    return \"\\n\".join(resumen)\n",
        "\n",
        "def construir_system_prompt(contexto: str) -> str:\n",
        "    return (\n",
        "        \"Rol: Analista de operaciones de una aerolinea.\\n\"\n",
        "        \"Objetivo: Responder preguntas de negocio con base en metricas.\\n\"\n",
        "        \"Restricciones: No revelar razonamiento interno. No usar emojis.\\n\"\n",
        "        \"Formato de salida: JSON con llaves summary, insights, risks, actions, metrics_used.\\n\"\n",
        "        \"Ejemplo de salida:\\n\"\n",
        "        \"{\\n\"\n",
        "        \"  \\\"summary\\\": \\\"La tasa de escalamiento es elevada en ciertos estados.\\\",\\n\"\n",
        "        \"  \\\"insights\\\": [\\\"Los hilos con mas mensajes muestran mayor escalamiento.\\\"],\\n\"\n",
        "        \"  \\\"risks\\\": [\\\"Falsos negativos impactan la satisfaccion del cliente.\\\"],\\n\"\n",
        "        \"  \\\"actions\\\": [\\\"Priorizar mejora en los estados con mayor tasa.\\\"],\\n\"\n",
        "        \"  \\\"metrics_used\\\": [\\\"tasa_escalamiento\\\", \\\"mensajes_promedio\\\"]\\n\"\n",
        "        \"}\\n\\n\"\n",
        "        f\"Contexto:\\n{contexto}\"\n",
        "    )\n",
        "\n",
        "def _safe_datetime_series(series: pd.Series) -> pd.Series:\n",
        "    if pd.api.types.is_datetime64_any_dtype(series):\n",
        "        return series\n",
        "    return pd.to_datetime(series.map(lambda x: x.get(\"$date\") if isinstance(x, dict) else x), errors=\"coerce\")\n",
        "\n",
        "\n",
        "def _get_conversation_text(df: pd.DataFrame, thread_id: Optional[str]) -> str:\n",
        "    if not thread_id or \"thread_id\" not in df.columns:\n",
        "        return \"\"\n",
        "\n",
        "    match = df[df[\"thread_id\"].astype(str) == str(thread_id)]\n",
        "    if match.empty:\n",
        "        return \"\"\n",
        "\n",
        "    if \"full_conversation\" in df.columns:\n",
        "        return str(match[\"full_conversation\"].iloc[0])\n",
        "\n",
        "    text_cols = [c for c in [\"content\", \"message_text\", \"text\", \"body\"] if c in df.columns]\n",
        "    if text_cols:\n",
        "        return \"\\n\".join(match[text_cols[0]].astype(str).tolist())\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "# --- CORRECCIÓN 2: Clausura (Closure) para Tools ---\n",
        "# Evita depender de una variable global 'df_master' inestable.\n",
        "def build_tools(df: pd.DataFrame):\n",
        "    \n",
        "    @tool\n",
        "    def obtener_metricas_resumen() -> str:\n",
        "        \"\"\"Devuelve un resumen de metricas clave de la aerolinea.\"\"\"\n",
        "        return construir_resumen_contexto(df)\n",
        "\n",
        "    @tool\n",
        "    def top_hilos_por_mensajes(n: int = 5) -> str:\n",
        "        \"\"\"Devuelve los top hilos con mas mensajes y su conteo.\"\"\"\n",
        "        if \"msg_count\" not in df.columns:\n",
        "            return \"No existe la columna msg_count.\"\n",
        "        top_df = df[[\"thread_id\", \"msg_count\"]].sort_values(\"msg_count\", ascending=False).head(n)\n",
        "        return top_df.to_string(index=False)\n",
        "\n",
        "    @tool\n",
        "    def top_hilos_por_duracion(n: int = 5) -> str:\n",
        "        \"\"\"Devuelve los top hilos con mayor duracion (min).\"\"\"\n",
        "        if \"duration_minutes\" not in df.columns:\n",
        "            return \"No existe la columna duration_minutes.\"\n",
        "        top_df = df[[\"thread_id\", \"duration_minutes\"]].sort_values(\n",
        "            \"duration_minutes\", ascending=False\n",
        "        ).head(n)\n",
        "        return top_df.to_string(index=False)\n",
        "\n",
        "    @tool\n",
        "    def tasa_escalamiento_por_categoria(col: str = \"status\", top_n: int = 8) -> str:\n",
        "        \"\"\"Calcula la tasa de escalamiento por categoria (status, platform, source).\"\"\"\n",
        "        if col not in df.columns or \"escalate_conversation\" not in df.columns:\n",
        "            return f\"No existe la columna {col} o escalate_conversation.\"\n",
        "        tmp = df[[col, \"escalate_conversation\"]].copy()\n",
        "        tmp[\"escalate_conversation\"] = tmp[\"escalate_conversation\"].fillna(0).astype(int)\n",
        "        rate = (\n",
        "            tmp.groupby(col)[\"escalate_conversation\"]\n",
        "            .mean()\n",
        "            .sort_values(ascending=False)\n",
        "            .head(top_n)\n",
        "            .reset_index()\n",
        "        )\n",
        "        return rate.to_string(index=False)\n",
        "\n",
        "    @tool\n",
        "    def resumen_tiempos_operativos() -> str:\n",
        "        \"\"\"Devuelve estadisticas de duracion y volumen de mensajes.\"\"\"\n",
        "        if \"duration_minutes\" not in df.columns or \"msg_count\" not in df.columns:\n",
        "            return \"No existen columnas de duracion o mensajes.\"\n",
        "        stats: Dict[str, float] = {\n",
        "            \"duracion_promedio_min\": df[\"duration_minutes\"].mean(),\n",
        "            \"duracion_p90_min\": df[\"duration_minutes\"].quantile(0.90),\n",
        "            \"mensajes_promedio\": df[\"msg_count\"].mean(),\n",
        "            \"mensajes_p90\": df[\"msg_count\"].quantile(0.90),\n",
        "        }\n",
        "        return pd.DataFrame([stats]).to_string(index=False)\n",
        "\n",
        "    @tool\n",
        "    def tendencia_escalamiento_mensual() -> str:\n",
        "        \"\"\"Muestra la tendencia mensual de escalamiento.\"\"\"\n",
        "        if \"created_at\" not in df.columns or \"escalate_conversation\" not in df.columns:\n",
        "            return \"No existen columnas requeridas para tendencia mensual.\"\n",
        "        tmp = df[[\"created_at\", \"escalate_conversation\"]].copy()\n",
        "        tmp[\"created_at\"] = _safe_datetime_series(tmp[\"created_at\"])\n",
        "        tmp = tmp.dropna(subset=[\"created_at\"])\n",
        "        tmp[\"month\"] = tmp[\"created_at\"].dt.to_period(\"M\").astype(str)\n",
        "        tmp[\"escalate_conversation\"] = tmp[\"escalate_conversation\"].fillna(0).astype(int)\n",
        "        trend = tmp.groupby(\"month\")[\"escalate_conversation\"].mean().reset_index()\n",
        "        return trend.to_string(index=False)\n",
        "\n",
        "    @tool\n",
        "    def obtener_conversacion_por_thread(thread_id: str) -> str:\n",
        "        \"\"\"Devuelve la conversacion completa para un thread_id.\"\"\"\n",
        "        text = _get_conversation_text(df, thread_id)\n",
        "        if not text:\n",
        "            return \"No se encontro conversacion para ese thread_id.\"\n",
        "        return text\n",
        "\n",
        "    return [\n",
        "        obtener_metricas_resumen, top_hilos_por_mensajes, top_hilos_por_duracion,\n",
        "        tasa_escalamiento_por_categoria, resumen_tiempos_operativos, tendencia_escalamiento_mensual,\n",
        "        obtener_conversacion_por_thread\n",
        "    ]\n",
        "\n",
        "# Referencia al dataframe para los nodos del grafo (evita globals dispersos)\n",
        "_DF_REF: Optional[pd.DataFrame] = None\n",
        "\n",
        "# Inicializamos las tools usando el df_master (que debe existir en el notebook)\n",
        "# Si no existe, usamos una lista vacia para evitar error de definición\n",
        "try:\n",
        "    _DF_REF = df_master\n",
        "    TOOLS = build_tools(df_master)\n",
        "except NameError:\n",
        "    TOOLS = []\n",
        "\n",
        "LLM_WITH_TOOLS = LLM.bind_tools(TOOLS) if TOOLS else LLM\n",
        "\n",
        "MAX_TOOL_CALLS = 3\n",
        "\n",
        "# --- CORRECCIÓN 6: Tipado Fuerte ---\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], add_messages]\n",
        "    tool_calls_count: int\n",
        "    thread_id: Optional[str]\n",
        "    route: Optional[str]\n",
        "    analysis: Dict[str, str]\n",
        "\n",
        "def _parse_json_response(text: str) -> Dict[str, str]:\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except (json.JSONDecodeError, ValueError, TypeError):\n",
        "        return {}\n",
        "\n",
        "\n",
        "def nodo_router(state: AgentState) -> AgentState:\n",
        "    if \"messages\" not in state or not state[\"messages\"]:\n",
        "        raise ValueError(\"Estado invalido: messages vacio\")\n",
        "\n",
        "    thread_id = state.get(\"thread_id\")\n",
        "    conversation_text = _get_conversation_text(_DF_REF, thread_id) if _DF_REF is not None else \"\"\n",
        "\n",
        "    router_prompt = (\n",
        "        \"Rol: Analista de conversaciones automatizado.\\n\"\n",
        "        \"Tarea: Clasificar sentimiento y motivo, y decidir la ruta.\\n\"\n",
        "        \"Reglas de ruteo:\\n\"\n",
        "        \"- Si el sentimiento es negativo o hay queja fuerte: route = escalar\\n\"\n",
        "        \"- Si la consulta es tecnica: route = tecnico\\n\"\n",
        "        \"- Si la conversacion parece resuelta: route = resumen\\n\"\n",
        "        \"- Si no hay thread_id o conversacion: route = assistant\\n\"\n",
        "        \"Devuelve SOLO JSON con llaves: route, sentiment, motivo, rationale.\\n\"\n",
        "    )\n",
        "\n",
        "    last_user = state[\"messages\"][-1].content if hasattr(state[\"messages\"][-1], \"content\") else \"\"\n",
        "    router_input = (\n",
        "        f\"thread_id: {thread_id}\\n\"\n",
        "        f\"conversacion:\\n{conversation_text}\\n\\n\"\n",
        "        f\"pregunta_actual:\\n{last_user}\"\n",
        "    )\n",
        "\n",
        "    response = LLM.invoke([\n",
        "        SystemMessage(content=router_prompt),\n",
        "        HumanMessage(content=router_input),\n",
        "    ])\n",
        "\n",
        "    analysis = _parse_json_response(getattr(response, \"content\", \"\"))\n",
        "    route = analysis.get(\"route\", \"assistant\")\n",
        "\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [response],\n",
        "        \"tool_calls_count\": state.get(\"tool_calls_count\", 0),\n",
        "        \"thread_id\": thread_id,\n",
        "        \"route\": route,\n",
        "        \"analysis\": analysis,\n",
        "    }\n",
        "\n",
        "\n",
        "def nodo_escalar_humano(state: AgentState) -> AgentState:\n",
        "    analysis = state.get(\"analysis\", {})\n",
        "    thread_id = state.get(\"thread_id\")\n",
        "    conversation_text = _get_conversation_text(_DF_REF, thread_id) if _DF_REF is not None else \"\"\n",
        "\n",
        "    prompt = (\n",
        "        \"Rol: Analista de conversaciones automatizado.\\n\"\n",
        "        \"Tarea: Generar salida de escalamiento a humano.\\n\"\n",
        "        \"Devuelve SOLO JSON con llaves: action, sentiment, motivo, summary, next_steps.\\n\"\n",
        "        \"action debe ser 'escalar_humano'.\"\n",
        "    )\n",
        "\n",
        "    response = LLM.invoke([\n",
        "        SystemMessage(content=prompt),\n",
        "        HumanMessage(content=f\"analisis: {analysis}\\n\\nconversacion:\\n{conversation_text}\"),\n",
        "    ])\n",
        "\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [response],\n",
        "        \"tool_calls_count\": state.get(\"tool_calls_count\", 0),\n",
        "    }\n",
        "\n",
        "\n",
        "def nodo_respuesta_tecnica(state: AgentState) -> AgentState:\n",
        "    analysis = state.get(\"analysis\", {})\n",
        "    thread_id = state.get(\"thread_id\")\n",
        "    conversation_text = _get_conversation_text(_DF_REF, thread_id) if _DF_REF is not None else \"\"\n",
        "\n",
        "    prompt = (\n",
        "        \"Rol: Analista de conversaciones automatizado.\\n\"\n",
        "        \"Tarea: Generar borrador de respuesta tecnica.\\n\"\n",
        "        \"Devuelve SOLO JSON con llaves: action, sentiment, motivo, respuesta, assumptions.\\n\"\n",
        "        \"action debe ser 'borrador_tecnico'.\"\n",
        "    )\n",
        "\n",
        "    response = LLM.invoke([\n",
        "        SystemMessage(content=prompt),\n",
        "        HumanMessage(content=f\"analisis: {analysis}\\n\\nconversacion:\\n{conversation_text}\"),\n",
        "    ])\n",
        "\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [response],\n",
        "        \"tool_calls_count\": state.get(\"tool_calls_count\", 0),\n",
        "    }\n",
        "\n",
        "\n",
        "def nodo_resumen_resuelto(state: AgentState) -> AgentState:\n",
        "    analysis = state.get(\"analysis\", {})\n",
        "    thread_id = state.get(\"thread_id\")\n",
        "    conversation_text = _get_conversation_text(_DF_REF, thread_id) if _DF_REF is not None else \"\"\n",
        "\n",
        "    prompt = (\n",
        "        \"Rol: Analista de conversaciones automatizado.\\n\"\n",
        "        \"Tarea: Generar resumen final de conversacion resuelta.\\n\"\n",
        "        \"Devuelve SOLO JSON con llaves: action, sentiment, motivo, summary, resolution.\\n\"\n",
        "        \"action debe ser 'resumen'.\"\n",
        "    )\n",
        "\n",
        "    response = LLM.invoke([\n",
        "        SystemMessage(content=prompt),\n",
        "        HumanMessage(content=f\"analisis: {analysis}\\n\\nconversacion:\\n{conversation_text}\"),\n",
        "    ])\n",
        "\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [response],\n",
        "        \"tool_calls_count\": state.get(\"tool_calls_count\", 0),\n",
        "    }\n",
        "\n",
        "\n",
        "def nodo_asistente(state: AgentState) -> AgentState:\n",
        "    # --- CORRECCIÓN 3: Validación de Input ---\n",
        "    if \"messages\" not in state or not state[\"messages\"]:\n",
        "        raise ValueError(\"Estado invalido: messages vacio\")\n",
        "\n",
        "    try:\n",
        "        response = LLM_WITH_TOOLS.invoke(\n",
        "            state[\"messages\"] + [\n",
        "                SystemMessage(content=\"IMPORTANTE: Devuelve SOLO un JSON válido con las claves solicitadas.\")\n",
        "            ]\n",
        "        )\n",
        "    except Exception as exc:\n",
        "        # --- CORRECCIÓN 4: Logging ---\n",
        "        logger.error(f\"Error invocando LLM: {exc}\")\n",
        "        raise AgentError(f\"Fallo del LLM: {exc}\") from exc\n",
        "\n",
        "    tool_calls_count = state.get(\"tool_calls_count\", 0)\n",
        "    if hasattr(response, \"tool_calls\") and response.tool_calls:\n",
        "        tool_calls_count += 1\n",
        "\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [response],\n",
        "        \"tool_calls_count\": tool_calls_count,\n",
        "        \"thread_id\": state.get(\"thread_id\"),\n",
        "        \"route\": state.get(\"route\"),\n",
        "        \"analysis\": state.get(\"analysis\", {}),\n",
        "    }\n",
        "\n",
        "def construir_grafo() -> StateGraph:\n",
        "    tools_node = ToolNode(TOOLS)\n",
        "\n",
        "    def route_after_router(state: AgentState) -> str:\n",
        "        \"\"\"Decide el siguiente nodo segun la ruta del router.\"\"\"\n",
        "        ruta = state.get(\"route\", \"assistant\")\n",
        "        if ruta == \"escalar\":\n",
        "            return \"escalar\"\n",
        "        if ruta == \"tecnico\":\n",
        "            return \"tecnico\"\n",
        "        if ruta == \"resumen\":\n",
        "            return \"resumen\"\n",
        "        return \"assistant\"\n",
        "\n",
        "    def route_after_assistant(state: AgentState):\n",
        "        \"\"\"Decide si el asistente necesita tools o termina.\"\"\"\n",
        "        if state.get(\"tool_calls_count\", 0) >= MAX_TOOL_CALLS:\n",
        "            return END\n",
        "\n",
        "        last_msg = state[\"messages\"][-1]\n",
        "        if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n",
        "            return \"tools\"\n",
        "        return END\n",
        "\n",
        "    graph = StateGraph(AgentState)\n",
        "\n",
        "    # Nodos\n",
        "    graph.add_node(\"router\", nodo_router)\n",
        "    graph.add_node(\"assistant\", nodo_asistente)\n",
        "    graph.add_node(\"tools\", tools_node)\n",
        "    graph.add_node(\"escalar\", nodo_escalar_humano)\n",
        "    graph.add_node(\"tecnico\", nodo_respuesta_tecnica)\n",
        "    graph.add_node(\"resumen\", nodo_resumen_resuelto)\n",
        "\n",
        "    # Entry point: el router analiza y decide la ruta\n",
        "    graph.set_entry_point(\"router\")\n",
        "\n",
        "    # Router -> ruta especializada o asistente general\n",
        "    graph.add_conditional_edges(\n",
        "        \"router\",\n",
        "        route_after_router,\n",
        "        {\n",
        "            \"escalar\": \"escalar\",\n",
        "            \"tecnico\": \"tecnico\",\n",
        "            \"resumen\": \"resumen\",\n",
        "            \"assistant\": \"assistant\",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Nodos especializados -> END\n",
        "    graph.add_edge(\"escalar\", END)\n",
        "    graph.add_edge(\"tecnico\", END)\n",
        "    graph.add_edge(\"resumen\", END)\n",
        "\n",
        "    # Asistente general -> tools loop\n",
        "    graph.add_conditional_edges(\n",
        "        \"assistant\",\n",
        "        route_after_assistant,\n",
        "        {\"tools\": \"tools\", END: END},\n",
        "    )\n",
        "    graph.add_edge(\"tools\", \"assistant\")\n",
        "\n",
        "    return graph\n",
        "\n",
        "GRAPH_TEMPLATE = construir_grafo()\n",
        "AGENT_APPS: Dict[str, Any] = {}\n",
        "\n",
        "def get_agent_app(session_id: str):\n",
        "    if session_id not in AGENT_APPS:\n",
        "        AGENT_APPS[session_id] = GRAPH_TEMPLATE.compile(checkpointer=MemorySaver())\n",
        "    return AGENT_APPS[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810afcc6",
      "metadata": {},
      "source": [
        "## 7.A Memoria semantica FAISS \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e537a56a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from memory.faiss_memory import FaissMemory\n",
        "\n",
        "logging.getLogger(\"faiss.loader\").setLevel(logging.WARNING)\n",
        "\n",
        "MEMORY_ENABLED = True\n",
        "MEMORY = None\n",
        "\n",
        "try:\n",
        "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "    MEMORY = FaissMemory(\n",
        "        index_path=Path(\"outputs\") / \"faiss.index\",\n",
        "        metadata_path=Path(\"outputs\") / \"faiss_metadata.json\",\n",
        "        embeddings=embeddings,\n",
        "    )\n",
        "except Exception as exc:\n",
        "    MEMORY_ENABLED = False\n",
        "    MEMORY = None\n",
        "    logger.warning(\"Memoria FAISS deshabilitada: %s\", exc)\n",
        "\n",
        "\n",
        "def get_semantic_context(query: str, k: int = 3) -> str:\n",
        "    \"\"\"Recupera contexto semantico relevante. Devuelve string listo para prompt.\"\"\"\n",
        "    if not MEMORY_ENABLED or MEMORY is None or not query:\n",
        "        return \"\"\n",
        "\n",
        "    hits = MEMORY.search(query, k=k)\n",
        "    if not hits:\n",
        "        return \"\"\n",
        "\n",
        "    return \"\\n\".join([f\"- {h['text']}\" for h in hits])\n",
        "\n",
        "\n",
        "def save_semantic_memory(\n",
        "    question: str,\n",
        "    answer: str,\n",
        "    thread_id: str | None = None,\n",
        "    session_id: str | None = None,\n",
        ") -> None:\n",
        "    \"\"\"Guarda un par pregunta-respuesta en memoria semantica.\"\"\"\n",
        "    if not MEMORY_ENABLED or MEMORY is None or not answer:\n",
        "        return\n",
        "\n",
        "    MEMORY.save(\n",
        "        text=f\"Pregunta: {question}\\nRespuesta: {answer}\",\n",
        "        metadata={\"thread_id\": thread_id, \"session_id\": session_id},\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88a0f3d4",
      "metadata": {},
      "source": [
        "## 8. Ejecucion del agente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3108b0c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    if \"df_master\" not in globals():\n",
        "        if \"df_threads\" in globals() and \"df_messages\" in globals():\n",
        "            df_master = crear_tabla_maestra(df_threads, df_messages)\n",
        "        else:\n",
        "            df_master_path = Path(\"outputs\") / \"df_master.parquet\"\n",
        "            if df_master_path.exists():\n",
        "                df_master = pd.read_parquet(df_master_path)\n",
        "            else:\n",
        "                raise DataValidationError(\n",
        "                    \"df_master no esta cargado. Ejecuta la seccion de tabla maestra primero.\"\n",
        "                )\n",
        "\n",
        "    contexto = construir_resumen_contexto(df_master)\n",
        "    system_prompt = construir_system_prompt(contexto)\n",
        "\n",
        "    session_id = \"aerya-demo\"\n",
        "    app = get_agent_app(session_id)\n",
        "\n",
        "    thread_id = None\n",
        "    if \"thread_id\" in df_master.columns and not df_master.empty:\n",
        "        thread_id = str(df_master[\"thread_id\"].iloc[0])\n",
        "\n",
        "    user_question = \"Cual es la tasa de escalamiento por estado y cuales son los hilos con mayor duracion?\"\n",
        "\n",
        "    semantic_context = get_semantic_context(user_question, k=3)\n",
        "    if semantic_context:\n",
        "        system_prompt = f\"{system_prompt}\\n\\nContexto semantico relevante:\\n{semantic_context}\"\n",
        "\n",
        "    print(\"Iniciando flujo del agente...\")\n",
        "    for event in app.stream(\n",
        "        {\n",
        "            \"messages\": [\n",
        "                SystemMessage(content=system_prompt),\n",
        "                HumanMessage(content=user_question),\n",
        "            ],\n",
        "            \"tool_calls_count\": 0,\n",
        "            \"thread_id\": thread_id,\n",
        "            \"route\": None,\n",
        "            \"analysis\": {},\n",
        "        },\n",
        "        config={\"configurable\": {\"thread_id\": session_id}, \"recursion_limit\": 6},\n",
        "    ):\n",
        "        for value in event.values():\n",
        "            if \"messages\" in value:\n",
        "                last_msg = value[\"messages\"][-1]\n",
        "                if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n",
        "                    print(\"\\nAgente solicita herramientas:\")\n",
        "                    for tool_call in last_msg.tool_calls:\n",
        "                        print(f\"  - {tool_call['name']}\")\n",
        "                elif hasattr(last_msg, \"content\") and last_msg.content:\n",
        "                    if last_msg.content.strip():\n",
        "                        print(\"\\nRespuesta:\")\n",
        "                        print(last_msg.content)\n",
        "\n",
        "                        try:\n",
        "                            json.loads(last_msg.content)\n",
        "                            save_semantic_memory(\n",
        "                                question=user_question,\n",
        "                                answer=last_msg.content,\n",
        "                                thread_id=thread_id,\n",
        "                                session_id=session_id,\n",
        "                            )\n",
        "                        except (json.JSONDecodeError, ValueError):\n",
        "                            logger.debug(\"Respuesta no es JSON valido, no se guarda en memoria.\")\n",
        "\n",
        "except AgentError as exc:\n",
        "    logger.error(\"Error en el agente: %s\", exc)\n",
        "    fallback = {\n",
        "        \"summary\": \"No fue posible consultar el agente en este momento.\",\n",
        "        \"insights\": [],\n",
        "        \"risks\": [\"La respuesta se entrega en modo degradado.\"],\n",
        "        \"actions\": [\"Reintentar la consulta o validar la clave de API.\"],\n",
        "        \"metrics_used\": [],\n",
        "    }\n",
        "    print(\"Fallback JSON:\")\n",
        "    print(json.dumps(fallback, indent=2, ensure_ascii=False))\n",
        "except Exception as exc:\n",
        "    logger.error(\"Error inesperado en el agente: %s\", exc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "590ea9de",
      "metadata": {},
      "source": [
        "## 8.1 Evaluacion de calidad del agente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f0e6d3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import uuid\n",
        "\n",
        "\n",
        "EXPECTED_KEYS = {\"summary\", \"insights\", \"risks\", \"actions\", \"metrics_used\"}\n",
        "\n",
        "_JSON_REGEX = re.compile(r\"\\{[\\s\\S]*\\}\")\n",
        "\n",
        "\n",
        "def _extract_json(text: str) -> Optional[str]:\n",
        "    \"\"\"Extrae el primer bloque JSON de un texto, incluso si tiene texto alrededor.\"\"\"\n",
        "    match = _JSON_REGEX.search(text)\n",
        "    return match.group(0) if match else None\n",
        "\n",
        "\n",
        "def evaluar_respuesta_agente(raw_response: str) -> Dict[str, Any]:\n",
        "    \"\"\"Evalua la calidad de una respuesta del agente.\"\"\"\n",
        "    result: Dict[str, Any] = {\n",
        "        \"json_valido\": False,\n",
        "        \"claves_presentes\": [],\n",
        "        \"claves_faltantes\": [],\n",
        "        \"score_estructura\": 0.0,\n",
        "        \"insights_no_vacios\": False,\n",
        "        \"actions_no_vacios\": False,\n",
        "    }\n",
        "\n",
        "    json_str = _extract_json(raw_response) if raw_response else None\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(json_str) if json_str else json.loads(raw_response)\n",
        "        result[\"json_valido\"] = True\n",
        "    except (json.JSONDecodeError, ValueError, TypeError):\n",
        "        result[\"claves_faltantes\"] = list(EXPECTED_KEYS)\n",
        "        return result\n",
        "\n",
        "    presentes = EXPECTED_KEYS.intersection(parsed.keys())\n",
        "    faltantes = EXPECTED_KEYS - presentes\n",
        "\n",
        "    result[\"claves_presentes\"] = sorted(presentes)\n",
        "    result[\"claves_faltantes\"] = sorted(faltantes)\n",
        "    result[\"score_estructura\"] = len(presentes) / len(EXPECTED_KEYS)\n",
        "\n",
        "    if \"insights\" in parsed and isinstance(parsed[\"insights\"], list):\n",
        "        result[\"insights_no_vacios\"] = len(parsed[\"insights\"]) > 0\n",
        "\n",
        "    if \"actions\" in parsed and isinstance(parsed[\"actions\"], list):\n",
        "        result[\"actions_no_vacios\"] = len(parsed[\"actions\"]) > 0\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def ejecutar_evaluacion_agente(\n",
        "    preguntas: List[str],\n",
        "    df: pd.DataFrame,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Ejecuta multiples preguntas en sesiones aisladas y evalua las respuestas.\"\"\"\n",
        "    resultados = []\n",
        "\n",
        "    contexto = construir_resumen_contexto(df)\n",
        "    system_prompt = construir_system_prompt(contexto)\n",
        "\n",
        "    for pregunta in preguntas:\n",
        "        eval_session = f\"eval-{uuid.uuid4().hex[:8]}\"\n",
        "        eval_app = get_agent_app(eval_session)\n",
        "\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            res = eval_app.invoke(\n",
        "                {\n",
        "                    \"messages\": [\n",
        "                        SystemMessage(content=system_prompt),\n",
        "                        HumanMessage(content=pregunta),\n",
        "                    ],\n",
        "                    \"tool_calls_count\": 0,\n",
        "                    \"thread_id\": None,\n",
        "                    \"route\": None,\n",
        "                    \"analysis\": {},\n",
        "                },\n",
        "                config={\"configurable\": {\"thread_id\": eval_session}, \"recursion_limit\": 6},\n",
        "            )\n",
        "            answer = res[\"messages\"][-1].content\n",
        "        except Exception as exc:\n",
        "            answer = str(exc)\n",
        "\n",
        "        elapsed = time.time() - t0\n",
        "        evaluacion = evaluar_respuesta_agente(answer)\n",
        "        evaluacion[\"pregunta\"] = pregunta[:80]\n",
        "        evaluacion[\"tiempo_s\"] = round(elapsed, 2)\n",
        "        resultados.append(evaluacion)\n",
        "\n",
        "    return pd.DataFrame(resultados)\n",
        "\n",
        "\n",
        "# --- Ejecucion ---\n",
        "\n",
        "PREGUNTAS_EVAL = [\n",
        "    \"Que factores afectan el escalamiento?\",\n",
        "    \"Cual es la duracion promedio de las conversaciones?\",\n",
        "    \"Cuales son los estados con mayor tasa de escalamiento?\",\n",
        "]\n",
        "\n",
        "try:\n",
        "    df_eval = ejecutar_evaluacion_agente(PREGUNTAS_EVAL, df_master)\n",
        "\n",
        "    print(\"Agent Response Quality Check n\")\n",
        "    print(df_eval[[\"pregunta\", \"json_valido\", \"score_estructura\", \"tiempo_s\"]].to_string(index=False))\n",
        "    print(f\"\\nScore promedio de estructura: {df_eval['score_estructura'].mean():.0%}\")\n",
        "    print(f\"Respuestas JSON validas: {df_eval['json_valido'].sum()}/{len(df_eval)}\")\n",
        "    print(f\"Tiempo promedio: {df_eval['tiempo_s'].mean():.1f}s\")\n",
        "except Exception as exc:\n",
        "    logger.error(\"Error en evaluacion del agente: %s\", exc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f60d500",
      "metadata": {},
      "source": [
        "## 9. Persistencia opcional de resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7302a9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_DIR = Path(\"outputs\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "DF_MASTER_PATH = OUTPUT_DIR / \"df_master.parquet\"\n",
        "\n",
        "\n",
        "def cargar_df_master_si_existe(path: Path = DF_MASTER_PATH) -> pd.DataFrame | None:\n",
        "    if path.exists():\n",
        "        logger.info(\"Cargando desde: %s\", path)\n",
        "        return pd.read_parquet(path)\n",
        "    return None\n",
        "\n",
        "\n",
        "def obtener_df_master(\n",
        "    df_t: pd.DataFrame,\n",
        "    df_m: pd.DataFrame,\n",
        "    force_rebuild: bool = False,\n",
        "    persist: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    if not force_rebuild:\n",
        "        cached = cargar_df_master_si_existe()\n",
        "        if cached is not None:\n",
        "            return cached\n",
        "\n",
        "    df_master_local = crear_tabla_maestra(df_t, df_m)\n",
        "    if persist:\n",
        "        df_master_local.to_parquet(DF_MASTER_PATH, index=False)\n",
        "        logger.info(\"Guardado: %s\", DF_MASTER_PATH)\n",
        "    return df_master_local\n",
        "\n",
        "\n",
        "def guardar_metricas(df: pd.DataFrame, path_dir: Path = OUTPUT_DIR) -> None:\n",
        "    metrics_df = metricas_impacto_aerolinea(df)\n",
        "    metrics_df.to_csv(path_dir / \"metricas_impacto.csv\", index=False)\n",
        "    logger.info(\"Guardado: %s\", path_dir / \"metricas_impacto.csv\")\n",
        "\n",
        "\n",
        "def guardar_embeddings(df_embeddings: pd.DataFrame, path_dir: Path = OUTPUT_DIR) -> None:\n",
        "    df_embeddings.to_parquet(path_dir / \"df_embeddings.parquet\", index=False)\n",
        "    logger.info(\"Guardado: %s\", path_dir / \"df_embeddings.parquet\")\n",
        "\n",
        "\n",
        "# Uso recomendado:\n",
        "# df_master = obtener_df_master(df_threads, df_messages, force_rebuild=False, persist=False)\n",
        "# guardar_metricas(df_master)\n",
        "# guardar_embeddings(df_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ed1d15",
      "metadata": {},
      "source": [
        "## 10. API FastAPI (servicio en la nube)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a11f9e9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "import signal\n",
        "from contextlib import contextmanager\n",
        "\n",
        "from fastapi import FastAPI, Header, HTTPException, Request\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, ConfigDict, Field\n",
        "from slowapi import Limiter\n",
        "from slowapi.util import get_remote_address\n",
        "from slowapi.errors import RateLimitExceeded\n",
        "from slowapi.middleware import SlowAPIMiddleware\n",
        "\n",
        "\n",
        "# --- Config ---\n",
        "\n",
        "API_KEY = os.environ.get(\"AERYA_API_KEY\")\n",
        "if not API_KEY:\n",
        "    logger.warning(\"AERYA_API_KEY no definida. Usando key de desarrollo.\")\n",
        "    API_KEY = \"dev-key-local\"\n",
        "\n",
        "MAX_QUESTION_LENGTH = 2000\n",
        "AGENT_TIMEOUT_S = 30\n",
        "\n",
        "\n",
        "# --- Modelos Pydantic ---\n",
        "\n",
        "class AskRequest(BaseModel):\n",
        "    question: str = Field(..., min_length=1, max_length=MAX_QUESTION_LENGTH)\n",
        "    session_id: str | None = Field(None, max_length=128)\n",
        "    thread_id: str | None = None\n",
        "\n",
        "\n",
        "class AskResponse(BaseModel):\n",
        "    answer: str\n",
        "    session_id: str\n",
        "\n",
        "\n",
        "class HealthResponse(BaseModel):\n",
        "    status: str\n",
        "    graph_loaded: bool\n",
        "    memory_enabled: bool\n",
        "    llm_reachable: bool\n",
        "    active_sessions: int\n",
        "\n",
        "\n",
        "class MetricsResponse(BaseModel):\n",
        "    model_config = ConfigDict(extra=\"allow\")\n",
        "\n",
        "\n",
        "class ErrorResponse(BaseModel):\n",
        "    error: str\n",
        "\n",
        "\n",
        "# --- App ---\n",
        "\n",
        "limiter = Limiter(key_func=get_remote_address)\n",
        "\n",
        "api = FastAPI(\n",
        "    title=\"Aerya API\",\n",
        "    description=\"API para el agente de analisis de conversaciones de la aerolinea.\",\n",
        "    version=\"1.0.0\",\n",
        ")\n",
        "\n",
        "api.state.limiter = limiter\n",
        "api.add_middleware(SlowAPIMiddleware)\n",
        "api.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "# --- Timeout helper ---\n",
        "\n",
        "class AgentTimeoutError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def timeout(seconds: int):\n",
        "    \"\"\"Timeout context manager (solo Unix). En Windows se usa como fallback sin efecto.\"\"\"\n",
        "    if hasattr(signal, \"SIGALRM\"):\n",
        "        def handler(signum, frame):\n",
        "            raise AgentTimeoutError(f\"El agente excedio el timeout de {seconds}s.\")\n",
        "        old = signal.signal(signal.SIGALRM, handler)\n",
        "        signal.alarm(seconds)\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            signal.alarm(0)\n",
        "            signal.signal(signal.SIGALRM, old)\n",
        "    else:\n",
        "        yield\n",
        "\n",
        "\n",
        "# --- Dependencia: validacion de API key ---\n",
        "\n",
        "def validate_api_key(x_api_key: str = Header(..., alias=\"X-API-Key\")) -> str:\n",
        "    if x_api_key != API_KEY:\n",
        "        raise HTTPException(status_code=401, detail=\"API key invalida o ausente.\")\n",
        "    return x_api_key\n",
        "\n",
        "\n",
        "# --- Endpoints ---\n",
        "\n",
        "@api.get(\"/health\", response_model=HealthResponse)\n",
        "def health_check():\n",
        "    llm_ok = True\n",
        "    try:\n",
        "        LLM.invoke(\"ping\")\n",
        "    except Exception:\n",
        "        llm_ok = False\n",
        "\n",
        "    return HealthResponse(\n",
        "        status=\"ok\" if llm_ok else \"degraded\",\n",
        "        graph_loaded=GRAPH_TEMPLATE is not None,\n",
        "        memory_enabled=MEMORY_ENABLED,\n",
        "        llm_reachable=llm_ok,\n",
        "        active_sessions=len(AGENT_APPS),\n",
        "    )\n",
        "\n",
        "\n",
        "@api.get(\"/metrics\", response_model=MetricsResponse)\n",
        "def get_metrics(x_api_key: str = Header(..., alias=\"X-API-Key\")):\n",
        "    validate_api_key(x_api_key)\n",
        "    try:\n",
        "        metrics_df = metricas_impacto_aerolinea(df_master)\n",
        "        return metrics_df.to_dict(orient=\"records\")[0]\n",
        "    except Exception as exc:\n",
        "        logger.error(\"Error en /metrics\", extra={\"error\": str(exc)})\n",
        "        raise HTTPException(status_code=500, detail=str(exc))\n",
        "\n",
        "\n",
        "@api.post(\"/agent/ask\", response_model=AskResponse)\n",
        "@limiter.limit(\"10/minute\")\n",
        "def agent_ask(request: Request, req: AskRequest, x_api_key: str = Header(..., alias=\"X-API-Key\")):\n",
        "    validate_api_key(x_api_key)\n",
        "\n",
        "    session_id = req.session_id or str(uuid.uuid4())\n",
        "\n",
        "    logger.info(\n",
        "        \"agent_request\",\n",
        "        extra={\"session_id\": session_id, \"thread_id\": req.thread_id},\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        contexto = construir_resumen_contexto(df_master)\n",
        "        system_prompt = construir_system_prompt(contexto)\n",
        "\n",
        "        semantic_context = get_semantic_context(req.question, k=3)\n",
        "        if semantic_context:\n",
        "            system_prompt = f\"{system_prompt}\\n\\nContexto semantico relevante:\\n{semantic_context}\"\n",
        "\n",
        "        app_graph = get_agent_app(session_id)\n",
        "\n",
        "        with timeout(AGENT_TIMEOUT_S):\n",
        "            result = app_graph.invoke(\n",
        "                {\n",
        "                    \"messages\": [\n",
        "                        SystemMessage(content=system_prompt),\n",
        "                        HumanMessage(content=req.question),\n",
        "                    ],\n",
        "                    \"tool_calls_count\": 0,\n",
        "                    \"thread_id\": req.thread_id,\n",
        "                    \"route\": None,\n",
        "                    \"analysis\": {},\n",
        "                },\n",
        "                config={\"configurable\": {\"thread_id\": session_id}, \"recursion_limit\": 6},\n",
        "            )\n",
        "\n",
        "        answer = result[\"messages\"][-1].content\n",
        "\n",
        "        try:\n",
        "            json.loads(answer)\n",
        "            save_semantic_memory(\n",
        "                question=req.question,\n",
        "                answer=answer,\n",
        "                thread_id=req.thread_id,\n",
        "                session_id=session_id,\n",
        "            )\n",
        "        except (json.JSONDecodeError, ValueError):\n",
        "            logger.debug(\"Respuesta no es JSON valido, no se guarda en memoria.\")\n",
        "\n",
        "        logger.info(\n",
        "            \"agent_response\",\n",
        "            extra={\"session_id\": session_id, \"answer_length\": len(answer)},\n",
        "        )\n",
        "\n",
        "        return AskResponse(answer=answer, session_id=session_id)\n",
        "\n",
        "    except AgentTimeoutError:\n",
        "        logger.warning(\"Timeout en /agent/ask\", extra={\"session_id\": session_id})\n",
        "        raise HTTPException(status_code=504, detail=f\"El agente excedio el timeout de {AGENT_TIMEOUT_S}s.\")\n",
        "\n",
        "    except Exception as exc:\n",
        "        logger.error(\n",
        "            \"Error en /agent/ask\",\n",
        "            extra={\"session_id\": session_id, \"error\": str(exc)},\n",
        "        )\n",
        "        raise HTTPException(status_code=500, detail=str(exc))\n",
        "\n",
        "\n",
        "# Para ejecutar localmente:\n",
        "# uvicorn app:api --host 0.0.0.0 --port 8080 --reload\n",
        "\n",
        "# TODO produccion:\n",
        "# - Memoria externa con Redis: RedisSaver(redis_url)\n",
        "# - Estructura modular: app/api.py, app/agent.py, app/tools.py, app/config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f49951a1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e8ff4413",
      "metadata": {},
      "source": [
        "## 12. Servidor local (demo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53a741d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import uvicorn\n",
        "\n",
        "config = uvicorn.Config(api, host=\"0.0.0.0\", port=8000)\n",
        "server = uvicorn.Server(config)\n",
        "\n",
        "print(\"API activa en: http://localhost:8000/docs\")\n",
        "await server.serve()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Mioti)",
      "language": "python",
      "name": "mioti"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
